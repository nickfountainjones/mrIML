#' Wrapper to calculate performance metrics (Mathews correlation coefficient, sensitivity and specificity) for each model for each response variable.
#' @param yhats A \code{list} is the list generated by mrIMLpredicts
#' @param Model A \code{list}  the model used to generate the yhats object
#' @param Y  A \code{dataframe} is a response variable data set (species, SNPs etc).
#' @param mode \code{character}'classification' or 'regression' i.e., is the generative model a regression or classification?
#' @examples
#' \dontrun{
#' ModelPerf <- mrIMLperformance(yhats, Model = model1, Y = Y)
#' }
#'
#' @details Outputs a dataframe of commonly used metric that can be used to compare model performance of classification models. Performance metrics are based on testing data. But MCC is useful (higher numbers = better fit)
#'
#' @export

mrIMLperformance <- function(yhats, Model, Y, mode = "regression") {
  n_response <- length(yhats)
  mod_perf <- NULL
  bList <- yhats %>%
    purrr::map(
      purrr::pluck("last_mod_fit")
    )

  if (mode == "classification") {
    performance_function <- mrIMLperformance_classification
    global_metric <- "mcc"
  } else if (mode == "regression") {
    performance_function <- mrIMLperformance_classification
    global_metric <- "mcc"
  } else {
    stop(
      "mrIMLperfomance() currently only available for class \"regression\" or 
      \"classification\".",
      call. = FALSE
    )
  }
  
  model_perf <- performance_function(
    n_response,
    yhats,
    Y,
    Model,
    bList
  )
  
  global_summary <- model_perf[[global_metric]] %>%
    mean(na.rm = TRUE)
  
  return(
    list(
      model_performance = model_perf,
      global_performance_summary = global_summary
    ) 
  )

}

mrIMLperformance_classification <- function(n_response,
                                            yhats,
                                            Y,
                                            Model,
                                            bList) {
  m_perf <- lapply(
    1:n_response,
    function(i) {
      tibble(
        response = names(yhats)[i],
        model_name = class(Model)[1],
        roc_AUC = bList[[i]]$.metrics[[1]]$.estimate[2],
        mcc = bList[[i]]$.predictions[[1]] %>%
          yardstick::mcc(
            truth = class,
            estimate = .pred_class
          ) %>%
          dplyr::pull(.estimate),
        sensitivity = bList[[i]]$.predictions[[1]] %>%
          yardstick::sens(
            truth = class,
            estimate = .pred_class
          ) %>%
          dplyr::pull(.estimate),
        ppv = bList[[i]]$.predictions[[1]] %>%
          yardstick::ppv(
            truth = class,
            estimate = .pred_class
          ) %>%
          dplyr::pull(.estimate),
        specificity = bList[[i]]$.predictions[[1]] %>%
          yardstick::spec(
            truth = class,
            estimate = .pred_class
          ) %>%
          dplyr::pull(.estimate),
        prevalence = sum(Y[i]) / nrow(Y)
      )
    }
  ) %>%
    dplyr::bind_rows()
  
  # Handling for NAs in MCC
  if (any(is.na(m_perf$mcc))) {
    warning(
      paste0("NAs produced when calculating MCC. This is common when there ",
             "is a class imbalance. Substituting NA values with zero."),
      call. = FALSE
    )
    m_perf <- m_perf %>%
      mutate(
        mcc = ifelse(is.na(mcc), 0, mcc)
      )
  }
  
  m_perf

}

mrIMLperformance_regression <- function(n_response,
                                        yhats,
                                        Y,
                                        Model,
                                        bList) {
  lapply(
    1:n_response,
    function(i) {
      tibble(
        response = names(yhats)[i],
        model_name = class(Model)[1],
        rmse = bList[[i]]$.metrics[[1]]$.estimate[1],
        rsquared = bList[[i]]$.metrics[[1]]$.estimate[12]
      )
    }
  ) %>%
    dplyr::bind_rows()
}
